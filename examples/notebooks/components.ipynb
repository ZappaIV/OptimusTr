{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d811690f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from pathlib import Path\n",
    "import sys  \n",
    "\n",
    "# Get my_package directory path from Notebook\n",
    "parent_dir = str(Path().resolve().parents[1])\n",
    "\n",
    "# Add to sys.path\n",
    "sys.path.insert(0, parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b94284c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import math\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from src.transformers.models.functionals import create_cross_attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c58bca07",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 128\n",
    "num_heads = 8\n",
    "hidden_dim = 200\n",
    "max_len = 5000\n",
    "batch_size = 32\n",
    "d_ff = hidden_dim\n",
    "num_head = 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a69f131b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 7]) torch.Size([3, 8])\n",
      "torch.Size([3, 7, 128]) torch.Size([3, 8, 128])\n"
     ]
    }
   ],
   "source": [
    "def get_inputs_tensors():\n",
    "\n",
    "    src = torch.tensor(\n",
    "        [[ 1,3,4,2,0,0,0],\n",
    "        [ 1,3,4,4,2,0,0],\n",
    "        [ 1,2,0,0,0,0,0]]\n",
    "    ) # [ 3, 7]\n",
    "        \n",
    "    # en_tensor = torch.randint(1, 10**4,[batch_size, seq_len], dtype=int)\n",
    "    tgt = torch.tensor(\n",
    "        [[ 1,3,2,0,0,0,0,0],\n",
    "        [ 1,3,3,3,2,0,0,0],\n",
    "        [ 1,3,2,0,0,0,0,0]]\n",
    "    ) # [ 3, 8]\n",
    "\n",
    "    print(src.shape, tgt.shape)\n",
    "\n",
    "    embedding = nn.Embedding(10**4, embedding_dim=embed_dim)\n",
    "    src_embedding = embedding(src)\n",
    "    tgt_embedding = embedding(tgt)\n",
    "\n",
    "    print(src_embedding.shape, tgt_embedding.shape)\n",
    "    cross_padding_mask = create_cross_attention_mask(tgt, src)\n",
    "    \n",
    "    return tgt, src, src_embedding, tgt_embedding, cross_padding_mask\n",
    "\n",
    "\n",
    "tgt, src, src_embedding, tgt_embedding, cross_padding_mask = get_inputs_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d3c537c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_model_parameters(model: nn.Module):\n",
    "    print(model)\n",
    "    for p in model.parameters():\n",
    "        if p.requires_grad:\n",
    "            print(p.name, p.data)\n",
    "        else:\n",
    "            print(p.name)\n",
    "            print(p)\n",
    "\n",
    "def check_gradient_explosion(\n",
    "    model: nn.Module,\n",
    "    target: torch.Tensor, \n",
    "    forward_args: list,\n",
    "    forward_kwargs:dict\n",
    "):\n",
    "    torch.cuda.empty_cache()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.5)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(*forward_args, **forward_kwargs)\n",
    "    loss_value = criterion(outputs, target)\n",
    "    loss_value.backward()\n",
    "    \n",
    "    print(nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0))\n",
    "    optimizer.step()\n",
    "    \n",
    "    return outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2703674",
   "metadata": {},
   "source": [
    "# **Layers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d1c22ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EncoderLayer(\n",
      "  (self_attention): MultiHeadAttention(\n",
      "    (w_q): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (w_k): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (w_v): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (w_o): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (feed_forward): FeedForward(\n",
      "    (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (relu): ReLU()\n",
      "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "None tensor([[ 0.0283,  0.0774, -0.0439,  ..., -0.0065,  0.0478,  0.0393],\n",
      "        [ 0.0179,  0.0412, -0.0012,  ...,  0.0274, -0.0614,  0.0683],\n",
      "        [-0.0677, -0.0851, -0.0063,  ...,  0.0051, -0.0009,  0.0289],\n",
      "        ...,\n",
      "        [-0.0793,  0.0513, -0.0772,  ...,  0.0843,  0.0509,  0.0329],\n",
      "        [ 0.0773, -0.0482, -0.0150,  ..., -0.0669, -0.0161, -0.0112],\n",
      "        [-0.0466, -0.0473, -0.0855,  ..., -0.0172, -0.0008, -0.0360]])\n",
      "None tensor([ 0.0208, -0.0791,  0.0605,  0.0259,  0.0514, -0.0783,  0.0747,  0.0198,\n",
      "        -0.0508,  0.0638, -0.0198,  0.0377, -0.0621, -0.0645, -0.0764, -0.0493,\n",
      "         0.0091,  0.0636, -0.0561, -0.0036,  0.0141,  0.0870,  0.0801,  0.0853,\n",
      "        -0.0539,  0.0868,  0.0047, -0.0565,  0.0042,  0.0197, -0.0820, -0.0443,\n",
      "         0.0718, -0.0230, -0.0220,  0.0844, -0.0221,  0.0291,  0.0602,  0.0590,\n",
      "        -0.0055, -0.0550,  0.0304, -0.0651, -0.0380,  0.0130,  0.0606,  0.0636,\n",
      "         0.0382,  0.0522,  0.0813, -0.0694, -0.0223, -0.0354,  0.0243, -0.0800,\n",
      "        -0.0101, -0.0141, -0.0662, -0.0820,  0.0634, -0.0055, -0.0723,  0.0753,\n",
      "        -0.0573,  0.0614,  0.0063,  0.0361, -0.0729, -0.0614,  0.0491,  0.0289,\n",
      "         0.0012,  0.0445, -0.0741, -0.0810, -0.0403,  0.0571,  0.0055, -0.0323,\n",
      "        -0.0694,  0.0881, -0.0403,  0.0445, -0.0350,  0.0087,  0.0076,  0.0798,\n",
      "        -0.0861,  0.0370,  0.0066,  0.0276,  0.0389, -0.0613,  0.0476, -0.0016,\n",
      "        -0.0847, -0.0054,  0.0692, -0.0032,  0.0663,  0.0764,  0.0155, -0.0194,\n",
      "        -0.0308, -0.0584,  0.0346, -0.0751, -0.0348, -0.0109,  0.0867,  0.0269,\n",
      "        -0.0505,  0.0152, -0.0712, -0.0377, -0.0586,  0.0571, -0.0391,  0.0780,\n",
      "         0.0181,  0.0206,  0.0664, -0.0679,  0.0256,  0.0727, -0.0143, -0.0482])\n",
      "None tensor([[ 0.0123, -0.0450,  0.0567,  ..., -0.0237, -0.0749,  0.0368],\n",
      "        [-0.0810, -0.0063,  0.0186,  ...,  0.0573, -0.0519,  0.0620],\n",
      "        [-0.0812, -0.0788, -0.0594,  ..., -0.0426, -0.0122, -0.0049],\n",
      "        ...,\n",
      "        [-0.0707, -0.0386,  0.0200,  ..., -0.0732,  0.0340, -0.0136],\n",
      "        [-0.0542,  0.0431,  0.0836,  ..., -0.0510, -0.0762, -0.0453],\n",
      "        [-0.0370,  0.0561, -0.0320,  ...,  0.0592, -0.0882, -0.0401]])\n",
      "None tensor([-0.0719, -0.0584,  0.0012, -0.0147, -0.0547, -0.0120, -0.0729, -0.0276,\n",
      "         0.0524,  0.0485,  0.0594,  0.0566, -0.0480,  0.0124,  0.0745, -0.0275,\n",
      "        -0.0350,  0.0343, -0.0768,  0.0221, -0.0448,  0.0144,  0.0557,  0.0411,\n",
      "         0.0747, -0.0485,  0.0408,  0.0737,  0.0183, -0.0786,  0.0180,  0.0148,\n",
      "         0.0546,  0.0617,  0.0538,  0.0099,  0.0140,  0.0047, -0.0674, -0.0679,\n",
      "         0.0215, -0.0347, -0.0734,  0.0001, -0.0079,  0.0004,  0.0346,  0.0024,\n",
      "        -0.0389,  0.0271,  0.0389,  0.0645, -0.0025,  0.0650,  0.0672,  0.0619,\n",
      "         0.0219, -0.0594,  0.0016, -0.0533,  0.0712,  0.0335,  0.0567, -0.0018,\n",
      "        -0.0085, -0.0713,  0.0127,  0.0358,  0.0647, -0.0176,  0.0222, -0.0507,\n",
      "         0.0426,  0.0259, -0.0671,  0.0877,  0.0116,  0.0195,  0.0135, -0.0789,\n",
      "         0.0859, -0.0582, -0.0475, -0.0250, -0.0280,  0.0498,  0.0757, -0.0561,\n",
      "         0.0490, -0.0730,  0.0577, -0.0597, -0.0133, -0.0750, -0.0331,  0.0607,\n",
      "         0.0055,  0.0343,  0.0196,  0.0665,  0.0254, -0.0220, -0.0116,  0.0003,\n",
      "         0.0621, -0.0382, -0.0666, -0.0328, -0.0037, -0.0102, -0.0014,  0.0058,\n",
      "         0.0203, -0.0755,  0.0250, -0.0746,  0.0724,  0.0035,  0.0442,  0.0704,\n",
      "         0.0100, -0.0639,  0.0602,  0.0785,  0.0878,  0.0234, -0.0170, -0.0590])\n",
      "None tensor([[ 0.0714, -0.0556,  0.0680,  ..., -0.0030, -0.0220, -0.0312],\n",
      "        [ 0.0716,  0.0098,  0.0866,  ...,  0.0103,  0.0323, -0.0143],\n",
      "        [-0.0515,  0.0837,  0.0364,  ...,  0.0275, -0.0007,  0.0601],\n",
      "        ...,\n",
      "        [ 0.0173, -0.0845, -0.0163,  ..., -0.0735, -0.0546,  0.0150],\n",
      "        [-0.0743,  0.0665,  0.0152,  ...,  0.0272, -0.0395,  0.0491],\n",
      "        [-0.0494,  0.0307,  0.0120,  ...,  0.0770, -0.0107,  0.0388]])\n",
      "None tensor([ 7.1832e-02,  3.9341e-02, -5.8792e-02, -1.3748e-02, -3.0078e-02,\n",
      "         3.5659e-02, -7.1504e-02, -3.0263e-02, -6.1701e-02, -5.8920e-02,\n",
      "         1.5297e-02, -7.1930e-02, -5.0526e-02, -8.5165e-02,  1.7600e-02,\n",
      "        -3.1015e-03, -2.0429e-02, -6.5252e-02, -3.0010e-02,  1.9801e-02,\n",
      "        -7.6774e-02,  8.5254e-03, -8.6918e-02, -7.2620e-02, -1.3822e-03,\n",
      "        -1.8289e-02,  3.9593e-02,  7.1801e-02, -6.4799e-02,  5.4952e-02,\n",
      "        -1.5938e-02,  1.9280e-03,  9.8631e-03,  2.6661e-02,  8.0512e-02,\n",
      "         3.7517e-02, -6.0687e-02, -7.1910e-02, -7.3930e-02,  6.6645e-02,\n",
      "        -3.4384e-03,  5.4945e-02,  6.7331e-05,  7.1721e-02, -7.4212e-02,\n",
      "        -2.7254e-02,  3.3551e-02, -6.3378e-02, -5.7171e-03, -5.0338e-02,\n",
      "        -8.3400e-02, -4.8478e-03,  4.3184e-02, -5.9615e-03, -1.7445e-02,\n",
      "        -1.8907e-02,  6.1678e-02, -6.0051e-02,  2.3425e-02,  3.8398e-02,\n",
      "        -8.4593e-02, -7.0632e-02,  8.5005e-02, -5.4836e-02, -6.3193e-02,\n",
      "        -8.2891e-02,  1.9953e-02, -7.6866e-03, -8.3531e-02,  7.5512e-02,\n",
      "         6.2707e-03, -1.5410e-02, -1.6262e-02, -6.9747e-02,  8.6479e-02,\n",
      "        -3.6218e-02,  7.3564e-02,  3.6568e-02,  9.4891e-03,  1.6999e-02,\n",
      "        -5.0115e-02, -1.8798e-02, -4.5695e-02, -7.8879e-03,  3.5301e-02,\n",
      "         6.5198e-02, -6.4374e-02,  6.2155e-02, -9.5635e-03,  8.0718e-02,\n",
      "         6.7382e-02,  6.1221e-02, -5.7366e-02, -1.6123e-02, -8.7895e-02,\n",
      "         3.4243e-03,  1.1826e-02, -4.6747e-02,  7.7738e-02,  4.7864e-02,\n",
      "         4.2099e-02,  6.7423e-02, -3.4377e-03, -2.2374e-02,  6.3322e-02,\n",
      "        -2.0147e-02, -5.1882e-03, -6.8130e-02,  1.3586e-02,  8.2348e-02,\n",
      "        -8.0144e-02, -2.9181e-02, -4.7925e-02, -3.4083e-02,  2.4764e-02,\n",
      "         6.8214e-02,  8.5304e-02, -8.0960e-02,  2.1893e-02,  6.7147e-02,\n",
      "         4.6341e-02,  2.0834e-02, -7.3299e-03, -5.6553e-02, -4.7664e-02,\n",
      "        -1.7806e-02, -5.6274e-02,  2.3570e-02])\n",
      "None tensor([[ 0.0809, -0.0827,  0.0045,  ..., -0.0348, -0.0758,  0.0759],\n",
      "        [-0.0538, -0.0832,  0.0543,  ...,  0.0140, -0.0096,  0.0441],\n",
      "        [ 0.0540, -0.0458,  0.0266,  ..., -0.0158, -0.0526,  0.0619],\n",
      "        ...,\n",
      "        [-0.0456,  0.0240,  0.0690,  ..., -0.0327, -0.0249,  0.0802],\n",
      "        [ 0.0515,  0.0335, -0.0744,  ...,  0.0406,  0.0540,  0.0588],\n",
      "        [-0.0637, -0.0861,  0.0556,  ..., -0.0045, -0.0155, -0.0156]])\n",
      "None tensor([ 7.2958e-02, -4.4735e-02,  4.7801e-02, -2.3500e-02, -1.3164e-03,\n",
      "         6.7135e-02,  4.2337e-03, -2.6083e-02, -4.2537e-02,  4.8788e-02,\n",
      "         2.2062e-02,  2.5382e-02, -1.3565e-02,  3.9756e-02,  8.3585e-02,\n",
      "         6.2609e-02,  4.3974e-02, -5.2720e-02, -6.2687e-02, -9.2334e-04,\n",
      "        -9.3931e-03,  4.6919e-02, -5.5841e-02,  8.3433e-02, -4.9146e-02,\n",
      "        -8.3831e-02, -2.4811e-02, -6.8539e-03, -1.0574e-02,  2.8500e-02,\n",
      "        -8.6273e-02, -3.2306e-02,  5.8763e-05,  3.8633e-02,  7.6473e-02,\n",
      "         8.3221e-02,  1.4385e-03, -4.6167e-02, -3.8360e-02, -1.9298e-03,\n",
      "        -2.6196e-02, -8.6501e-03, -8.0936e-02,  4.5003e-02, -8.2945e-02,\n",
      "        -1.9650e-04, -4.6536e-02, -6.9584e-02,  5.8147e-02,  6.2337e-02,\n",
      "         2.7087e-02, -3.4711e-02, -3.9061e-03,  5.5912e-02, -2.8171e-02,\n",
      "        -5.9879e-03,  1.1851e-02, -6.5214e-02,  2.0494e-02,  7.8287e-02,\n",
      "         7.2141e-02, -2.1677e-03,  8.3687e-02,  2.0002e-02,  7.2583e-02,\n",
      "        -7.8559e-02,  3.2039e-02, -7.3400e-02,  7.1697e-02,  4.9918e-02,\n",
      "        -7.1920e-02, -3.9044e-02, -4.9325e-03, -7.9015e-02, -1.6466e-02,\n",
      "        -5.1905e-02,  1.5701e-03, -8.5656e-02,  1.4707e-04,  8.2900e-02,\n",
      "        -8.4717e-02,  7.8224e-03, -8.5577e-02, -2.5305e-02, -6.3400e-02,\n",
      "         7.9219e-02,  8.2549e-02, -8.7220e-02,  1.5861e-03,  6.4200e-02,\n",
      "        -2.4095e-02,  3.1577e-02,  7.8273e-02, -8.2374e-02, -5.0703e-02,\n",
      "         8.2146e-02,  4.0335e-02,  8.2769e-02, -3.2314e-02,  8.5436e-02,\n",
      "         7.4523e-02, -9.4896e-04, -5.7258e-02,  7.9634e-02, -5.0325e-02,\n",
      "         1.3658e-02, -7.1882e-02, -3.3400e-02, -1.8903e-02, -4.8201e-02,\n",
      "        -4.8577e-02,  3.9620e-03, -6.0621e-02, -5.9250e-02,  7.0844e-02,\n",
      "        -2.2654e-02,  7.9734e-02, -5.4336e-02, -4.2994e-02,  3.6296e-02,\n",
      "        -3.5297e-02,  6.9587e-02, -4.4677e-02, -6.5165e-03,  1.4268e-02,\n",
      "        -4.0026e-02, -4.6183e-02,  4.3666e-02])\n",
      "None tensor([[-0.0739, -0.0260,  0.0547,  ...,  0.0665, -0.0829,  0.0517],\n",
      "        [ 0.0619, -0.0852,  0.0195,  ..., -0.0057,  0.0348, -0.0113],\n",
      "        [ 0.0867,  0.0315,  0.0663,  ..., -0.0142, -0.0417, -0.0212],\n",
      "        ...,\n",
      "        [ 0.0360, -0.0188,  0.0636,  ..., -0.0851,  0.0062, -0.0363],\n",
      "        [-0.0073,  0.0128,  0.0095,  ..., -0.0348,  0.0566, -0.0135],\n",
      "        [-0.0098,  0.0559, -0.0481,  ..., -0.0422,  0.0172,  0.0800]])\n",
      "None tensor([ 0.0522, -0.0875, -0.0730, -0.0012,  0.0027, -0.0069, -0.0866,  0.0335,\n",
      "        -0.0552, -0.0716,  0.0300, -0.0688,  0.0826, -0.0591, -0.0070,  0.0392,\n",
      "         0.0705, -0.0118, -0.0333,  0.0047,  0.0416,  0.0732,  0.0734, -0.0163,\n",
      "         0.0413, -0.0063, -0.0042,  0.0163, -0.0581, -0.0239, -0.0539,  0.0581,\n",
      "        -0.0271, -0.0394, -0.0144, -0.0233,  0.0071, -0.0763, -0.0581, -0.0696,\n",
      "        -0.0271, -0.0485,  0.0719,  0.0267, -0.0002, -0.0094,  0.0528,  0.0716,\n",
      "         0.0049, -0.0571, -0.0609,  0.0625,  0.0576, -0.0480, -0.0243,  0.0741,\n",
      "         0.0430, -0.0416,  0.0873,  0.0008, -0.0502,  0.0646, -0.0775, -0.0488,\n",
      "         0.0779, -0.0831, -0.0064,  0.0107, -0.0016,  0.0768,  0.0863, -0.0849,\n",
      "         0.0512,  0.0366,  0.0836, -0.0129, -0.0508, -0.0014,  0.0854,  0.0690,\n",
      "        -0.0025,  0.0544,  0.0310, -0.0376, -0.0708,  0.0195,  0.0091,  0.0140,\n",
      "        -0.0788, -0.0480,  0.0192,  0.0531, -0.0288, -0.0056,  0.0679,  0.0671,\n",
      "         0.0359,  0.0025, -0.0309,  0.0432,  0.0082, -0.0270, -0.0181,  0.0706,\n",
      "        -0.0239, -0.0317, -0.0059, -0.0321,  0.0348, -0.0006,  0.0613, -0.0175,\n",
      "        -0.0671,  0.0856, -0.0587,  0.0159, -0.0257,  0.0638,  0.0540,  0.0624,\n",
      "         0.0492, -0.0577, -0.0366,  0.0217, -0.0033, -0.0049,  0.0673,  0.0291])\n",
      "None tensor([[ 0.0329,  0.0657, -0.0267,  ..., -0.0513, -0.0670, -0.0400],\n",
      "        [-0.0653, -0.0795, -0.0429,  ..., -0.0084,  0.0183, -0.0256],\n",
      "        [ 0.0195,  0.0648,  0.0472,  ..., -0.0034,  0.0095, -0.0714],\n",
      "        ...,\n",
      "        [ 0.0280, -0.0694,  0.0865,  ..., -0.0789,  0.0596,  0.0273],\n",
      "        [ 0.0098,  0.0736, -0.0405,  ..., -0.0201,  0.0357, -0.0270],\n",
      "        [ 0.0205, -0.0373,  0.0176,  ..., -0.0458, -0.0157,  0.0490]])\n",
      "None tensor([ 0.0332, -0.0060,  0.0864, -0.0335,  0.0785,  0.0616,  0.0406, -0.0178,\n",
      "        -0.0764, -0.0468,  0.0577,  0.0662, -0.0772, -0.0845,  0.0719,  0.0297,\n",
      "         0.0720,  0.0597, -0.0156, -0.0405, -0.0427, -0.0253, -0.0605, -0.0035,\n",
      "         0.0796,  0.0689,  0.0832,  0.0111,  0.0424, -0.0108, -0.0427,  0.0303,\n",
      "         0.0739,  0.0270, -0.0333,  0.0041,  0.0650, -0.0327,  0.0210,  0.0656,\n",
      "         0.0192, -0.0874,  0.0512, -0.0318,  0.0353, -0.0213,  0.0122,  0.0389,\n",
      "         0.0633,  0.0261,  0.0764,  0.0625, -0.0427,  0.0075,  0.0808, -0.0578,\n",
      "        -0.0567,  0.0287,  0.0610, -0.0127, -0.0798,  0.0774, -0.0411, -0.0434,\n",
      "         0.0519, -0.0268, -0.0778, -0.0248, -0.0849,  0.0472, -0.0196, -0.0839,\n",
      "        -0.0151,  0.0869, -0.0881,  0.0874, -0.0383,  0.0634,  0.0084, -0.0799,\n",
      "        -0.0188,  0.0417, -0.0086, -0.0532, -0.0066, -0.0553, -0.0364,  0.0761,\n",
      "        -0.0521, -0.0382,  0.0206,  0.0300,  0.0427,  0.0524, -0.0607, -0.0451,\n",
      "        -0.0105,  0.0456, -0.0870, -0.0338, -0.0424,  0.0260, -0.0558,  0.0535,\n",
      "        -0.0148, -0.0049,  0.0674, -0.0269,  0.0676, -0.0071,  0.0316,  0.0814,\n",
      "        -0.0790,  0.0384, -0.0589, -0.0601, -0.0795, -0.0287, -0.0743, -0.0044,\n",
      "        -0.0691, -0.0386, -0.0071,  0.0173, -0.0821,  0.0640, -0.0874, -0.0014])\n",
      "None tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.])\n",
      "None tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "None tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.])\n",
      "None tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "None tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.])\n",
      "None tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "DecoderLayer(\n",
      "  (self_attention): MultiHeadAttention(\n",
      "    (w_q): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (w_k): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (w_v): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (w_o): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (cross_attention): MultiHeadAttention(\n",
      "    (w_q): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (w_k): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (w_v): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (w_o): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (feed_forward): FeedForward(\n",
      "    (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (relu): ReLU()\n",
      "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "None tensor([[ 0.0206, -0.0118, -0.0822,  ..., -0.0533,  0.0791,  0.0379],\n",
      "        [-0.0211,  0.0841, -0.0254,  ...,  0.0190, -0.0750, -0.0454],\n",
      "        [-0.0195, -0.0507, -0.0421,  ..., -0.0513,  0.0637, -0.0319],\n",
      "        ...,\n",
      "        [-0.0619,  0.0791,  0.0008,  ..., -0.0883, -0.0258,  0.0241],\n",
      "        [ 0.0056,  0.0812,  0.0844,  ...,  0.0535, -0.0870,  0.0654],\n",
      "        [ 0.0224, -0.0416,  0.0718,  ...,  0.0603,  0.0806, -0.0117]])\n",
      "None tensor([-0.0270,  0.0812,  0.0410,  0.0719, -0.0750, -0.0537, -0.0058, -0.0683,\n",
      "         0.0763,  0.0373, -0.0069, -0.0855,  0.0842,  0.0143,  0.0821, -0.0313,\n",
      "         0.0242,  0.0799,  0.0308,  0.0290,  0.0637, -0.0699,  0.0376,  0.0811,\n",
      "         0.0705, -0.0198,  0.0628, -0.0086,  0.0079,  0.0866,  0.0146, -0.0327,\n",
      "         0.0732,  0.0553, -0.0114, -0.0044,  0.0596,  0.0209, -0.0353, -0.0546,\n",
      "        -0.0849, -0.0300, -0.0796, -0.0454,  0.0035,  0.0197,  0.0680,  0.0053,\n",
      "        -0.0179,  0.0483,  0.0232,  0.0175, -0.0127,  0.0473, -0.0492, -0.0428,\n",
      "         0.0556,  0.0025,  0.0653,  0.0048, -0.0362, -0.0019, -0.0597, -0.0860,\n",
      "         0.0255, -0.0832,  0.0389,  0.0854,  0.0191, -0.0188, -0.0342, -0.0680,\n",
      "        -0.0436,  0.0765,  0.0426, -0.0498, -0.0438,  0.0240,  0.0349,  0.0499,\n",
      "         0.0296, -0.0497, -0.0321,  0.0015, -0.0534, -0.0429, -0.0357, -0.0156,\n",
      "        -0.0286, -0.0491, -0.0305,  0.0835, -0.0772, -0.0282,  0.0484, -0.0547,\n",
      "         0.0831,  0.0845, -0.0168, -0.0284, -0.0223,  0.0563,  0.0501, -0.0583,\n",
      "        -0.0149,  0.0068, -0.0180,  0.0051, -0.0073, -0.0497, -0.0872, -0.0314,\n",
      "        -0.0525, -0.0817, -0.0446, -0.0735,  0.0726,  0.0748, -0.0676,  0.0179,\n",
      "         0.0425, -0.0693, -0.0322, -0.0581,  0.0875, -0.0471,  0.0684, -0.0524])\n",
      "None tensor([[-0.0565, -0.0804, -0.0379,  ...,  0.0105,  0.0874,  0.0215],\n",
      "        [-0.0621, -0.0589,  0.0003,  ..., -0.0086, -0.0415,  0.0162],\n",
      "        [-0.0025,  0.0526,  0.0073,  ..., -0.0749,  0.0493,  0.0815],\n",
      "        ...,\n",
      "        [-0.0479,  0.0119,  0.0046,  ..., -0.0288,  0.0175, -0.0160],\n",
      "        [ 0.0718, -0.0503,  0.0829,  ...,  0.0533,  0.0545, -0.0461],\n",
      "        [-0.0426,  0.0022,  0.0538,  ...,  0.0781,  0.0769, -0.0542]])\n",
      "None tensor([ 0.0578, -0.0161,  0.0765, -0.0591, -0.0293,  0.0056, -0.0668,  0.0034,\n",
      "         0.0458,  0.0409, -0.0355, -0.0636, -0.0601, -0.0691, -0.0696, -0.0396,\n",
      "        -0.0433, -0.0418, -0.0840,  0.0299,  0.0527,  0.0715, -0.0346, -0.0576,\n",
      "        -0.0822,  0.0636, -0.0305, -0.0597, -0.0401,  0.0664, -0.0401,  0.0155,\n",
      "         0.0700,  0.0150,  0.0668, -0.0821,  0.0044,  0.0593,  0.0594, -0.0807,\n",
      "         0.0630, -0.0033, -0.0012,  0.0398,  0.0551, -0.0662,  0.0447,  0.0088,\n",
      "        -0.0013,  0.0064,  0.0734,  0.0054, -0.0370,  0.0349, -0.0585, -0.0466,\n",
      "         0.0779,  0.0537,  0.0522, -0.0808,  0.0799,  0.0426,  0.0679,  0.0179,\n",
      "        -0.0721,  0.0041, -0.0529,  0.0382,  0.0005,  0.0732, -0.0811, -0.0079,\n",
      "         0.0408, -0.0535, -0.0232,  0.0589, -0.0588, -0.0761,  0.0147,  0.0839,\n",
      "        -0.0687, -0.0733, -0.0451,  0.0307,  0.0803,  0.0015, -0.0042,  0.0421,\n",
      "         0.0040, -0.0335, -0.0395, -0.0019, -0.0066, -0.0521, -0.0238, -0.0377,\n",
      "        -0.0579, -0.0189, -0.0065,  0.0188, -0.0400,  0.0063, -0.0168,  0.0350,\n",
      "         0.0090, -0.0215, -0.0774,  0.0067, -0.0489,  0.0426, -0.0053, -0.0071,\n",
      "        -0.0285,  0.0236, -0.0408, -0.0484, -0.0167, -0.0059,  0.0160, -0.0790,\n",
      "        -0.0349, -0.0663, -0.0301,  0.0595,  0.0141, -0.0533, -0.0368, -0.0358])\n",
      "None tensor([[ 0.0053, -0.0001,  0.0693,  ..., -0.0275,  0.0715, -0.0756],\n",
      "        [-0.0605,  0.0534, -0.0855,  ...,  0.0631, -0.0091, -0.0257],\n",
      "        [-0.0852, -0.0064,  0.0244,  ...,  0.0377,  0.0590,  0.0845],\n",
      "        ...,\n",
      "        [-0.0541, -0.0262, -0.0445,  ..., -0.0813,  0.0518, -0.0836],\n",
      "        [ 0.0497,  0.0348,  0.0022,  ...,  0.0173,  0.0470,  0.0246],\n",
      "        [-0.0249,  0.0439,  0.0315,  ..., -0.0808,  0.0804, -0.0097]])\n",
      "None tensor([-0.0524, -0.0479, -0.0648,  0.0869, -0.0333, -0.0073,  0.0589, -0.0774,\n",
      "         0.0356, -0.0569,  0.0041,  0.0718, -0.0311, -0.0046,  0.0568,  0.0859,\n",
      "         0.0095, -0.0647,  0.0764,  0.0602,  0.0753, -0.0612, -0.0059,  0.0121,\n",
      "         0.0835, -0.0443,  0.0152, -0.0090, -0.0111,  0.0490,  0.0068,  0.0116,\n",
      "         0.0460,  0.0149, -0.0229,  0.0800,  0.0538,  0.0051,  0.0457,  0.0022,\n",
      "         0.0459, -0.0202,  0.0653, -0.0148, -0.0225, -0.0647,  0.0331, -0.0553,\n",
      "         0.0067,  0.0010, -0.0336, -0.0266, -0.0105, -0.0640, -0.0787, -0.0729,\n",
      "        -0.0332,  0.0284, -0.0628,  0.0474, -0.0523, -0.0578,  0.0685, -0.0418,\n",
      "        -0.0493, -0.0693,  0.0516, -0.0172, -0.0078,  0.0354,  0.0745, -0.0305,\n",
      "         0.0234, -0.0354, -0.0261, -0.0641,  0.0690, -0.0242, -0.0155, -0.0591,\n",
      "        -0.0718, -0.0470, -0.0273, -0.0522,  0.0584,  0.0823,  0.0355, -0.0307,\n",
      "        -0.0376,  0.0283, -0.0598, -0.0494, -0.0798, -0.0240,  0.0597, -0.0301,\n",
      "        -0.0583,  0.0878, -0.0499, -0.0696,  0.0817,  0.0211, -0.0132,  0.0207,\n",
      "        -0.0453, -0.0062, -0.0729,  0.0805,  0.0324, -0.0511,  0.0171,  0.0208,\n",
      "        -0.0382,  0.0781,  0.0302, -0.0675, -0.0446, -0.0860,  0.0469, -0.0097,\n",
      "        -0.0248,  0.0747, -0.0005,  0.0809, -0.0363,  0.0507, -0.0231,  0.0325])\n",
      "None tensor([[-0.0297,  0.0303,  0.0161,  ..., -0.0139,  0.0340, -0.0861],\n",
      "        [ 0.0670,  0.0551,  0.0112,  ...,  0.0277,  0.0178,  0.0662],\n",
      "        [-0.0687, -0.0644,  0.0349,  ..., -0.0340,  0.0649, -0.0771],\n",
      "        ...,\n",
      "        [ 0.0678, -0.0443, -0.0255,  ..., -0.0389, -0.0463, -0.0829],\n",
      "        [-0.0244, -0.0250,  0.0418,  ..., -0.0208,  0.0552,  0.0202],\n",
      "        [ 0.0463,  0.0830,  0.0585,  ...,  0.0131,  0.0879, -0.0271]])\n",
      "None tensor([ 0.0437, -0.0052,  0.0771, -0.0319, -0.0328,  0.0153, -0.0214, -0.0711,\n",
      "        -0.0679,  0.0544, -0.0179, -0.0008,  0.0476,  0.0812, -0.0104, -0.0440,\n",
      "         0.0476,  0.0104,  0.0454,  0.0795,  0.0628,  0.0603,  0.0474, -0.0341,\n",
      "        -0.0733, -0.0273, -0.0549,  0.0650, -0.0742,  0.0045, -0.0149,  0.0695,\n",
      "        -0.0861,  0.0236, -0.0241, -0.0559, -0.0313,  0.0224,  0.0816, -0.0171,\n",
      "         0.0410,  0.0161, -0.0297,  0.0864,  0.0420,  0.0131, -0.0084,  0.0306,\n",
      "         0.0796, -0.0818,  0.0072,  0.0561, -0.0639,  0.0057, -0.0140, -0.0441,\n",
      "        -0.0083,  0.0618, -0.0290, -0.0211, -0.0516,  0.0162, -0.0094, -0.0369,\n",
      "         0.0239, -0.0645, -0.0882, -0.0025,  0.0794,  0.0679, -0.0117, -0.0754,\n",
      "         0.0179,  0.0690,  0.0291, -0.0330,  0.0101, -0.0735,  0.0209,  0.0186,\n",
      "        -0.0633,  0.0273, -0.0481,  0.0201, -0.0479, -0.0694,  0.0572, -0.0500,\n",
      "        -0.0651, -0.0485, -0.0371, -0.0072,  0.0736,  0.0211,  0.0784, -0.0005,\n",
      "         0.0781,  0.0417, -0.0148, -0.0084,  0.0367, -0.0442,  0.0715, -0.0822,\n",
      "        -0.0783, -0.0512,  0.0154, -0.0605, -0.0465,  0.0698, -0.0323,  0.0388,\n",
      "        -0.0239,  0.0206, -0.0835, -0.0388, -0.0863,  0.0244, -0.0057,  0.0036,\n",
      "        -0.0610,  0.0674,  0.0313, -0.0664, -0.0009, -0.0028,  0.0011, -0.0582])\n",
      "None tensor([[-0.0296, -0.0035,  0.0498,  ..., -0.0779,  0.0114,  0.0789],\n",
      "        [-0.0403, -0.0072, -0.0773,  ..., -0.0017, -0.0782, -0.0298],\n",
      "        [-0.0068, -0.0074, -0.0144,  ..., -0.0797, -0.0623,  0.0700],\n",
      "        ...,\n",
      "        [-0.0880,  0.0150,  0.0634,  ..., -0.0802,  0.0846,  0.0633],\n",
      "        [-0.0092,  0.0027, -0.0311,  ...,  0.0400,  0.0612, -0.0652],\n",
      "        [-0.0705,  0.0140,  0.0202,  ..., -0.0105,  0.0293, -0.0850]])\n",
      "None tensor([-0.0704, -0.0515,  0.0284, -0.0024,  0.0369, -0.0139, -0.0252,  0.0672,\n",
      "        -0.0714,  0.0258,  0.0790,  0.0486,  0.0269,  0.0204, -0.0325,  0.0619,\n",
      "        -0.0392, -0.0532,  0.0104, -0.0442, -0.0853,  0.0529, -0.0041,  0.0140,\n",
      "        -0.0562, -0.0365,  0.0412, -0.0521,  0.0694,  0.0428,  0.0040, -0.0405,\n",
      "        -0.0337, -0.0515,  0.0351,  0.0679, -0.0791, -0.0122, -0.0635,  0.0251,\n",
      "        -0.0780, -0.0483, -0.0698, -0.0552,  0.0737,  0.0829,  0.0409,  0.0681,\n",
      "        -0.0493,  0.0812,  0.0815,  0.0473, -0.0702, -0.0127,  0.0267, -0.0432,\n",
      "        -0.0437,  0.0417, -0.0190, -0.0850, -0.0365,  0.0090,  0.0118, -0.0768,\n",
      "         0.0201, -0.0765, -0.0834,  0.0668,  0.0064,  0.0355,  0.0596, -0.0240,\n",
      "        -0.0410,  0.0711,  0.0367, -0.0292, -0.0522, -0.0096, -0.0756,  0.0204,\n",
      "        -0.0422,  0.0426,  0.0479,  0.0454,  0.0087, -0.0437, -0.0457, -0.0009,\n",
      "         0.0727, -0.0566,  0.0017,  0.0326,  0.0323,  0.0146, -0.0203,  0.0761,\n",
      "         0.0748, -0.0517, -0.0099,  0.0602, -0.0857,  0.0465,  0.0329,  0.0783,\n",
      "         0.0127,  0.0387,  0.0120,  0.0123, -0.0158, -0.0304,  0.0358,  0.0594,\n",
      "         0.0823,  0.0842,  0.0336, -0.0597, -0.0490,  0.0566, -0.0196,  0.0464,\n",
      "        -0.0369, -0.0599,  0.0744,  0.0101, -0.0825,  0.0647, -0.0145, -0.0307])\n",
      "None tensor([[ 0.0135,  0.0808,  0.0225,  ..., -0.0313, -0.0078,  0.0307],\n",
      "        [-0.0637,  0.0599,  0.0673,  ...,  0.0312,  0.0019, -0.0619],\n",
      "        [-0.0679,  0.0836, -0.0049,  ...,  0.0869, -0.0739,  0.0181],\n",
      "        ...,\n",
      "        [-0.0267,  0.0046,  0.0160,  ...,  0.0834, -0.0005,  0.0249],\n",
      "        [-0.0613,  0.0843, -0.0272,  ...,  0.0196, -0.0182, -0.0322],\n",
      "        [-0.0278,  0.0804, -0.0205,  ...,  0.0697, -0.0846, -0.0332]])\n",
      "None tensor([-0.0060, -0.0105, -0.0762,  0.0662,  0.0471,  0.0336,  0.0516, -0.0340,\n",
      "         0.0671,  0.0652, -0.0381,  0.0720, -0.0146,  0.0075,  0.0466,  0.0406,\n",
      "         0.0190, -0.0305,  0.0782, -0.0688,  0.0146,  0.0388, -0.0080,  0.0876,\n",
      "         0.0287,  0.0580,  0.0057, -0.0772, -0.0702,  0.0439, -0.0566, -0.0850,\n",
      "        -0.0330, -0.0241,  0.0797,  0.0071,  0.0213,  0.0066,  0.0239,  0.0363,\n",
      "        -0.0671, -0.0516,  0.0394,  0.0013,  0.0670,  0.0250, -0.0503, -0.0296,\n",
      "        -0.0120,  0.0458,  0.0226,  0.0233, -0.0276,  0.0218, -0.0433,  0.0131,\n",
      "        -0.0752, -0.0758,  0.0843,  0.0486, -0.0421,  0.0013,  0.0434, -0.0233,\n",
      "         0.0443,  0.0474, -0.0443,  0.0057,  0.0194, -0.0737, -0.0853, -0.0874,\n",
      "        -0.0790,  0.0766, -0.0732, -0.0430,  0.0335, -0.0099,  0.0400,  0.0505,\n",
      "        -0.0333, -0.0276,  0.0561,  0.0704,  0.0702,  0.0269,  0.0231, -0.0137,\n",
      "         0.0573,  0.0866, -0.0008,  0.0041,  0.0085,  0.0443, -0.0681,  0.0458,\n",
      "        -0.0686,  0.0320,  0.0413, -0.0003, -0.0619,  0.0396, -0.0689,  0.0383,\n",
      "         0.0320,  0.0530, -0.0054, -0.0740, -0.0348, -0.0170, -0.0537, -0.0035,\n",
      "         0.0351,  0.0859, -0.0329, -0.0383,  0.0667, -0.0688,  0.0218, -0.0536,\n",
      "        -0.0086, -0.0402,  0.0208,  0.0803, -0.0478, -0.0010, -0.0327, -0.0863])\n",
      "None tensor([[-0.0575,  0.0727, -0.0276,  ...,  0.0594,  0.0849, -0.0204],\n",
      "        [ 0.0521,  0.0597,  0.0302,  ...,  0.0824,  0.0281, -0.0845],\n",
      "        [ 0.0036,  0.0734,  0.0698,  ..., -0.0074,  0.0593, -0.0034],\n",
      "        ...,\n",
      "        [-0.0077,  0.0461,  0.0479,  ...,  0.0329, -0.0152, -0.0040],\n",
      "        [-0.0812,  0.0060,  0.0647,  ...,  0.0277, -0.0882, -0.0835],\n",
      "        [-0.0749, -0.0850, -0.0377,  ..., -0.0563,  0.0382, -0.0822]])\n",
      "None tensor([ 0.0191, -0.0868,  0.0358,  0.0196,  0.0127,  0.0345, -0.0068,  0.0222,\n",
      "        -0.0266,  0.0820, -0.0792,  0.0131,  0.0687, -0.0428, -0.0026,  0.0077,\n",
      "         0.0565,  0.0613, -0.0591,  0.0494,  0.0877,  0.0178, -0.0650,  0.0680,\n",
      "         0.0829, -0.0223, -0.0865, -0.0592,  0.0586,  0.0087,  0.0243, -0.0793,\n",
      "         0.0773,  0.0038,  0.0557,  0.0819,  0.0812,  0.0796, -0.0255, -0.0519,\n",
      "         0.0847, -0.0490, -0.0096, -0.0266, -0.0352,  0.0471,  0.0461, -0.0553,\n",
      "         0.0098, -0.0257, -0.0305, -0.0015, -0.0231,  0.0071,  0.0419, -0.0536,\n",
      "        -0.0009,  0.0085,  0.0193,  0.0592, -0.0333,  0.0307,  0.0269, -0.0410,\n",
      "         0.0650,  0.0361,  0.0653, -0.0586, -0.0377,  0.0165,  0.0087,  0.0564,\n",
      "         0.0425,  0.0079, -0.0254,  0.0772, -0.0528, -0.0427,  0.0193, -0.0008,\n",
      "        -0.0042,  0.0476,  0.0736, -0.0837, -0.0339,  0.0547,  0.0645, -0.0494,\n",
      "        -0.0389, -0.0374,  0.0121, -0.0479, -0.0876, -0.0176, -0.0867, -0.0323,\n",
      "         0.0464,  0.0036, -0.0572,  0.0840, -0.0371,  0.0814,  0.0846,  0.0097,\n",
      "         0.0237, -0.0452, -0.0736, -0.0316,  0.0238, -0.0274,  0.0195,  0.0701,\n",
      "        -0.0179,  0.0749,  0.0459,  0.0432,  0.0448, -0.0250, -0.0390,  0.0774,\n",
      "        -0.0018, -0.0732,  0.0859,  0.0287,  0.0827,  0.0276,  0.0510, -0.0263])\n",
      "None tensor([[ 0.0161,  0.0212,  0.0660,  ..., -0.0611,  0.0156,  0.0464],\n",
      "        [ 0.0566,  0.0103,  0.0238,  ..., -0.0566, -0.0286, -0.0456],\n",
      "        [ 0.0445,  0.0570,  0.0426,  ...,  0.0882,  0.0739, -0.0813],\n",
      "        ...,\n",
      "        [ 0.0449, -0.0796, -0.0559,  ..., -0.0351,  0.0695,  0.0291],\n",
      "        [ 0.0496,  0.0865, -0.0535,  ...,  0.0042, -0.0637,  0.0093],\n",
      "        [-0.0563,  0.0764, -0.0849,  ...,  0.0065, -0.0091,  0.0753]])\n",
      "None tensor([ 0.0467,  0.0036,  0.0330,  0.0632,  0.0765, -0.0204, -0.0112, -0.0179,\n",
      "         0.0645, -0.0012,  0.0850,  0.0318,  0.0047, -0.0566, -0.0106,  0.0201,\n",
      "        -0.0548, -0.0240,  0.0356, -0.0241,  0.0835,  0.0356,  0.0586, -0.0333,\n",
      "         0.0032, -0.0835,  0.0436, -0.0813, -0.0134,  0.0432, -0.0095,  0.0358,\n",
      "         0.0707, -0.0490, -0.0646, -0.0709,  0.0145, -0.0415, -0.0204, -0.0733,\n",
      "        -0.0258, -0.0364,  0.0666,  0.0692,  0.0237,  0.0397,  0.0578,  0.0248,\n",
      "        -0.0570, -0.0557,  0.0577, -0.0565,  0.0750,  0.0609,  0.0324,  0.0043,\n",
      "         0.0038, -0.0400,  0.0031,  0.0434, -0.0432,  0.0604, -0.0190,  0.0424,\n",
      "         0.0106, -0.0665, -0.0553, -0.0360,  0.0566,  0.0367, -0.0460, -0.0459,\n",
      "         0.0752, -0.0121, -0.0776,  0.0066, -0.0450,  0.0127, -0.0104,  0.0083,\n",
      "        -0.0513,  0.0570, -0.0078,  0.0873,  0.0481, -0.0517, -0.0197,  0.0356,\n",
      "        -0.0304,  0.0456,  0.0064,  0.0627,  0.0758,  0.0768, -0.0257,  0.0224,\n",
      "         0.0353,  0.0380,  0.0857, -0.0022,  0.0155, -0.0403, -0.0080,  0.0397,\n",
      "         0.0157,  0.0332,  0.0635,  0.0104,  0.0838,  0.0820,  0.0848, -0.0664,\n",
      "         0.0152,  0.0388,  0.0035, -0.0173,  0.0017,  0.0034,  0.0829,  0.0857,\n",
      "         0.0230,  0.0562, -0.0834, -0.0129, -0.0474,  0.0758, -0.0050,  0.0444])\n",
      "None tensor([[ 0.0805,  0.0858, -0.0873,  ...,  0.0398,  0.0541, -0.0472],\n",
      "        [ 0.0295,  0.0285,  0.0640,  ..., -0.0199,  0.0618,  0.0587],\n",
      "        [-0.0449,  0.0526,  0.0733,  ...,  0.0098, -0.0661,  0.0610],\n",
      "        ...,\n",
      "        [ 0.0039, -0.0134,  0.0623,  ..., -0.0091, -0.0808,  0.0385],\n",
      "        [ 0.0053,  0.0310,  0.0558,  ...,  0.0113, -0.0227, -0.0625],\n",
      "        [ 0.0242, -0.0645,  0.0193,  ..., -0.0600,  0.0779, -0.0179]])\n",
      "None tensor([ 0.0613,  0.0379,  0.0865, -0.0771,  0.0420,  0.0477, -0.0306,  0.0643,\n",
      "         0.0114,  0.0151, -0.0484, -0.0120, -0.0375,  0.0741,  0.0462,  0.0225,\n",
      "        -0.0726, -0.0384, -0.0184, -0.0271, -0.0472, -0.0365, -0.0163,  0.0156,\n",
      "        -0.0174,  0.0777,  0.0007, -0.0174, -0.0537,  0.0441, -0.0149,  0.0776,\n",
      "         0.0471,  0.0412,  0.0510, -0.0427,  0.0462,  0.0128, -0.0115,  0.0385,\n",
      "        -0.0201,  0.0600, -0.0259,  0.0290, -0.0654,  0.0489, -0.0428, -0.0048,\n",
      "         0.0297,  0.0609,  0.0682, -0.0399, -0.0033,  0.0336, -0.0190,  0.0547,\n",
      "         0.0497, -0.0720, -0.0779, -0.0565,  0.0876, -0.0493,  0.0231,  0.0012,\n",
      "         0.0308,  0.0758, -0.0294, -0.0180, -0.0505,  0.0213,  0.0504,  0.0624,\n",
      "         0.0238, -0.0040, -0.0646, -0.0577,  0.0011, -0.0798,  0.0793,  0.0545,\n",
      "         0.0132, -0.0877,  0.0097, -0.0625,  0.0635, -0.0194,  0.0516, -0.0246,\n",
      "        -0.0211,  0.0005, -0.0037, -0.0134, -0.0536, -0.0745,  0.0705, -0.0073,\n",
      "         0.0305, -0.0480, -0.0563, -0.0736,  0.0551, -0.0704,  0.0668, -0.0734,\n",
      "        -0.0206,  0.0326,  0.0395,  0.0816,  0.0409, -0.0470,  0.0139, -0.0038,\n",
      "        -0.0327, -0.0717,  0.0417, -0.0564, -0.0865,  0.0282,  0.0179, -0.0036,\n",
      "        -0.0828, -0.0351,  0.0419,  0.0360,  0.0353, -0.0487, -0.0831,  0.0169])\n",
      "None tensor([[ 0.0577, -0.0752,  0.0707,  ..., -0.0672, -0.0346, -0.0010],\n",
      "        [ 0.0231,  0.0700, -0.0207,  ..., -0.0702, -0.0607, -0.0551],\n",
      "        [ 0.0543, -0.0685, -0.0843,  ...,  0.0107, -0.0250, -0.0868],\n",
      "        ...,\n",
      "        [ 0.0395,  0.0293, -0.0507,  ..., -0.0483,  0.0393,  0.0126],\n",
      "        [ 0.0605,  0.0158,  0.0105,  ..., -0.0640, -0.0022,  0.0701],\n",
      "        [-0.0294, -0.0269,  0.0728,  ...,  0.0161, -0.0257, -0.0653]])\n",
      "None tensor([ 0.0222,  0.0601,  0.0006, -0.0019,  0.0272, -0.0702, -0.0143,  0.0531,\n",
      "        -0.0446,  0.0836,  0.0409,  0.0616, -0.0196,  0.0072,  0.0550,  0.0210,\n",
      "        -0.0451,  0.0558, -0.0075,  0.0148,  0.0331, -0.0566,  0.0337,  0.0052,\n",
      "        -0.0859,  0.0109, -0.0035,  0.0571,  0.0016,  0.0017,  0.0040,  0.0809,\n",
      "         0.0187, -0.0678, -0.0215,  0.0128, -0.0113, -0.0115,  0.0547,  0.0474,\n",
      "         0.0641,  0.0402, -0.0820, -0.0207, -0.0808, -0.0387, -0.0466,  0.0809,\n",
      "         0.0643,  0.0176,  0.0720,  0.0489,  0.0798, -0.0836,  0.0478, -0.0006,\n",
      "        -0.0331, -0.0268, -0.0129,  0.0173,  0.0303,  0.0395, -0.0395, -0.0513,\n",
      "        -0.0712,  0.0374,  0.0479, -0.0628, -0.0652,  0.0600, -0.0111, -0.0025,\n",
      "         0.0320,  0.0829, -0.0760,  0.0799, -0.0762, -0.0103, -0.0418,  0.0367,\n",
      "        -0.0470, -0.0537, -0.0143,  0.0475, -0.0643, -0.0612,  0.0737, -0.0221,\n",
      "        -0.0829,  0.0479, -0.0640, -0.0696, -0.0043, -0.0862,  0.0310, -0.0097,\n",
      "         0.0622, -0.0689,  0.0343, -0.0664, -0.0862, -0.0703,  0.0230,  0.0599,\n",
      "         0.0856, -0.0639, -0.0778, -0.0488, -0.0034,  0.0709, -0.0568,  0.0752,\n",
      "        -0.0027,  0.0255, -0.0112,  0.0497,  0.0034, -0.0852,  0.0549,  0.0317,\n",
      "        -0.0416,  0.0641,  0.0619,  0.0726,  0.0019, -0.0683,  0.0507,  0.0629])\n",
      "None tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.])\n",
      "None tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "None tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.])\n",
      "None tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "None tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.])\n",
      "None tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "None tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.])\n",
      "None tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "from src.transformers.models.layers import EncoderLayer, DecoderLayer\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "encoder_layer = EncoderLayer(\n",
    "    embed_dim=embed_dim,\n",
    "    num_heads=num_head,\n",
    ")    \n",
    "\n",
    "decoder_layer = DecoderLayer(\n",
    "    embed_dim=embed_dim,\n",
    "    num_heads=num_head,\n",
    ")  \n",
    "\n",
    "show_model_parameters(encoder_layer)\n",
    "show_model_parameters(decoder_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9547c982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 7]) torch.Size([3, 8])\n",
      "torch.Size([3, 7, 128]) torch.Size([3, 8, 128])\n",
      "tensor(0.7772)\n"
     ]
    }
   ],
   "source": [
    "encoder_layer = EncoderLayer(\n",
    "    embed_dim=embed_dim,\n",
    "    num_heads=num_head,\n",
    ")   \n",
    "\n",
    "tgt, src, src_embedding, tgt_embedding, cross_padding_mask = get_inputs_tensors()\n",
    "memory = check_gradient_explosion( \n",
    "                                  encoder_layer,\n",
    "                                  src_embedding, \n",
    "                                  [src_embedding],\n",
    "                                  {'self_is_causal': True}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "505e0795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 7]) torch.Size([3, 8])\n",
      "torch.Size([3, 7, 128]) torch.Size([3, 8, 128])\n",
      "tensor(1.0517)\n"
     ]
    }
   ],
   "source": [
    "decoder_layer = DecoderLayer(\n",
    "    embed_dim=embed_dim,\n",
    "    num_heads=num_head,\n",
    ")   \n",
    "tgt, src, src_embedding, tgt_embedding, cross_padding_mask = get_inputs_tensors()\n",
    "logits = check_gradient_explosion( \n",
    "                                  decoder_layer,\n",
    "                                  tgt_embedding, \n",
    "                                  [tgt_embedding, src_embedding],\n",
    "                                  {'self_is_causal': True,\n",
    "                                   'cross_padding_mask': None}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6540f7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "src_embedding.shape=torch.Size([3, 7, 128]) \n",
      "tgt_embedding.shape=torch.Size([3, 8, 128])\n",
      "memory.shape=torch.Size([3, 7, 128]) \n",
      "logits.shape=torch.Size([3, 8, 128])\n"
     ]
    }
   ],
   "source": [
    "logits = decoder_layer(\n",
    "    tgt_embedding,\n",
    "    memory,\n",
    "    self_is_causal = True,\n",
    "    cross_padding_mask = cross_padding_mask\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"\\n{src_embedding.shape=}\",\n",
    "    f\"\\n{tgt_embedding.shape=}\"\n",
    "    f\"\\n{memory.shape=}\",\n",
    "    f\"\\n{logits.shape=}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5e755a",
   "metadata": {},
   "source": [
    "# **Encoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7aa23407",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\francesco.paletta\\AppData\\Local\\anaconda3\\envs\\optimus\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder(\n",
      "  (embedding): Embedding(10000, 128)\n",
      "  (positional_encoding): PositionalEncoding()\n",
      "  (layers): ModuleList(\n",
      "    (0-1): 2 x EncoderLayer(\n",
      "      (self_attention): MultiHeadAttention(\n",
      "        (w_q): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (w_k): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (w_v): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (w_o): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): FeedForward(\n",
      "        (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (relu): ReLU()\n",
      "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from src.transformers.models.encoders import Encoder\n",
    "\n",
    "encoder = Encoder(\n",
    "    vocab_size = 10**4,\n",
    "    embed_dim=embed_dim,\n",
    "    num_heads=num_head,\n",
    "    n_layers=2,\n",
    "    max_len=5000,\n",
    ")\n",
    "\n",
    "print(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f6fd680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "memory.shape=torch.Size([3, 7, 128])\n"
     ]
    }
   ],
   "source": [
    "memory = encoder(\n",
    "    src,\n",
    "    src_padding_mask = None,\n",
    "    self_is_causal = True\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"\\n{memory.shape=}\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05daf44f",
   "metadata": {},
   "source": [
    "# **Decoders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7507b6ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder(\n",
      "  (embedding): Embedding(10000, 128)\n",
      "  (positional_encoding): PositionalEncoding()\n",
      "  (layers): ModuleList(\n",
      "    (0-1): 2 x DecoderLayer(\n",
      "      (self_attention): MultiHeadAttention(\n",
      "        (w_q): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (w_k): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (w_v): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (w_o): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (cross_attention): MultiHeadAttention(\n",
      "        (w_q): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (w_k): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (w_v): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (w_o): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): FeedForward(\n",
      "        (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (relu): ReLU()\n",
      "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from src.transformers.models.decoders import Decoder\n",
    "\n",
    "decoder = Decoder(\n",
    "    vocab_size = 10**4,\n",
    "    embed_dim=embed_dim,\n",
    "    num_heads=num_head,\n",
    "    n_layers=2,\n",
    "    max_len=5000,\n",
    ")\n",
    "\n",
    "print(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ec8fc0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "logits.shape=torch.Size([3, 8, 128])\n"
     ]
    }
   ],
   "source": [
    "logits = decoder(\n",
    "    tgt,\n",
    "    memory,\n",
    "    self_is_causal=True,\n",
    "    cross_padding_mask=cross_padding_mask\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"\\n{logits.shape=}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45c0e7d",
   "metadata": {},
   "source": [
    "# **Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec19c7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src_vocab_size=10000, tgt_vocab_size=14641\n",
      "OptimusTransformer(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(10000, 128)\n",
      "    (positional_encoding): PositionalEncoding()\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (self_attention): MultiHeadAttention(\n",
      "          (w_q): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (w_k): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (w_v): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (w_o): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (feed_forward): FeedForward(\n",
      "          (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (relu): ReLU()\n",
      "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (embedding): Embedding(14641, 128)\n",
      "    (positional_encoding): PositionalEncoding()\n",
      "    (layers): ModuleList(\n",
      "      (0): DecoderLayer(\n",
      "        (self_attention): MultiHeadAttention(\n",
      "          (w_q): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (w_k): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (w_v): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (w_o): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (cross_attention): MultiHeadAttention(\n",
      "          (w_q): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (w_k): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (w_v): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (w_o): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (feed_forward): FeedForward(\n",
      "          (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (relu): ReLU()\n",
      "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output_projection): Linear(in_features=128, out_features=14641, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from src.transformers.models.optimus_model import OptimusTransformer\n",
    "\n",
    "config = {\n",
    "    'src_vocab_size': 10**4,\n",
    "    'tgt_vocab_size': 11**4,\n",
    "    'embed_dim': 128,\n",
    "    'num_heads': 16,\n",
    "    'n_layers': 1,\n",
    "    'hidden_dim': 256,\n",
    "    'max_seq_length': 100,\n",
    "    'dropout': 0.1,\n",
    "    'batch_size': 32,\n",
    "    'num_epochs': 10,\n",
    "    'warmup_steps': 4000,\n",
    "    'label_smoothing': 0.1\n",
    "}\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = OptimusTransformer(\n",
    "src_vocab_size=config['src_vocab_size'],\n",
    "tgt_vocab_size=config['tgt_vocab_size'],\n",
    "n_layers=config['n_layers'],\n",
    "embed_dim=config['embed_dim'],\n",
    "num_heads=config['num_heads'],\n",
    "max_len=config['max_seq_length'],\n",
    "dropout=config['dropout'],\n",
    ").to(device)\n",
    "\n",
    "print(model)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ad29ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 8, 14641])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output = model(\n",
    "        src,\n",
    "        tgt,\n",
    "        cross_padding_mask = cross_padding_mask,\n",
    "        tgt_is_causal = True,\n",
    "        memory_is_causal = True\n",
    "    )\n",
    "\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f76092fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[False, False, False, False,  True,  True,  True]]],\n",
      "\n",
      "\n",
      "        [[[False, False, False, False, False,  True,  True]]],\n",
      "\n",
      "\n",
      "        [[[False, False,  True,  True,  True,  True,  True]]]])\n",
      "tensor(3362)\n"
     ]
    }
   ],
   "source": [
    "start_seq = torch.tensor([[1],[1],[1]])\n",
    "cross_padding_mask = create_cross_attention_mask(start_seq, src)\n",
    "print(cross_padding_mask)\n",
    "with torch.no_grad():\n",
    "    output = model(\n",
    "        src,\n",
    "        start_seq,\n",
    "        cross_padding_mask = cross_padding_mask,\n",
    "        tgt_is_causal = True,\n",
    "        memory_is_causal = True\n",
    "    )\n",
    "\n",
    "print(torch.argmax(output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5cd8a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test Forward Pass ===\n",
      "Input source shape: torch.Size([3, 7])\n",
      "Input target shape: torch.Size([3, 8])\n",
      "Output shape: torch.Size([3, 8, 14641])\n",
      "\n",
      "=== Test Generazione ===\n",
      "Generated sequence shape: torch.Size([3, 21])\n",
      "First generated sequence: [1, 11127, 11127, 11127, 11127, 11127, 11127, 11127, 11127, 11127, 11127, 11127, 11127, 11127, 11127, 11127, 11127, 11127, 11127, 11127, 11127]\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Test Forward Pass ===\")\n",
    "with torch.no_grad():\n",
    "    output = model(\n",
    "        src,\n",
    "        tgt,\n",
    "        cross_padding_mask = cross_padding_mask,\n",
    "        tgt_is_causal = True,\n",
    "        memory_is_causal = True\n",
    "    )\n",
    "    print(f\"Input source shape: {src.shape}\")\n",
    "    print(f\"Input target shape: {tgt.shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")  # [batch_size, tgt_len, vocab_size]\n",
    "\n",
    "# Test generazione\n",
    "print(\"\\n=== Test Generazione ===\")\n",
    "with torch.no_grad():\n",
    "    generated = model.generate(src, max_len=20, start_token=1, end_token=2)\n",
    "    print(f\"Generated sequence shape: {generated.shape}\")\n",
    "    print(f\"First generated sequence: {generated[0].tolist()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optimus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
