{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2c6eda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from pathlib import Path\n",
    "import sys  \n",
    "\n",
    "# Get my_package directory path from Notebook\n",
    "parent_dir = str(Path().resolve().parents[1])\n",
    "\n",
    "# Add to sys.path\n",
    "sys.path.insert(0, parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de897a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import math\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5c1cf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.components.functional import (generate_square_subsequent_mask, \n",
    "                               create_causal_mask,\n",
    "                               create_cross_attention_mask)\n",
    "\n",
    "def clear_nan(tensor: torch.Tensor):\n",
    "        return torch.where(torch.isnan(tensor), \n",
    "                               torch.zeros_like(tensor), \n",
    "                               tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fb89fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 128\n",
    "num_heads = 8\n",
    "hidden_dim = 200\n",
    "max_len = 5000\n",
    "batch_size = 32\n",
    "d_ff = hidden_dim\n",
    "num_head = 8\n",
    "src_seq_len = 4\n",
    "tgt_seq_len = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4dff56d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 7]) torch.Size([3, 1])\n",
      "torch.Size([3, 7, 128]) torch.Size([3, 1, 128])\n",
      "torch.Size([3, 8, 1, 16]) torch.Size([3, 8, 7, 16])\n"
     ]
    }
   ],
   "source": [
    "src = torch.tensor(\n",
    "    [[ 1,3,4,2,0,0,0],\n",
    "    [ 1,3,4,4,2,0,0],\n",
    "    [ 1,2,0,0,0,0,0]]\n",
    ")\n",
    "    \n",
    "# en_tensor = torch.randint(1, 10**4,[batch_size, seq_len], dtype=int)\n",
    "tgt = torch.tensor(\n",
    "    [[ 1],\n",
    "    [ 1],\n",
    "    [ 1]]\n",
    ")\n",
    "\n",
    "print(src.shape, tgt.shape)\n",
    "\n",
    "embedding = nn.Embedding(10**4, embedding_dim=embed_dim)\n",
    "src_embedding = embedding(src)\n",
    "tgt_embedding = embedding(tgt)\n",
    "\n",
    "print(src_embedding.shape, tgt_embedding.shape)\n",
    "\n",
    "batch_size, seq_len_src, embed_dim = src_embedding.shape\n",
    "_, seq_len_tgt, _ = tgt_embedding.shape\n",
    "\n",
    "linear = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "Q = linear(tgt_embedding).view(batch_size, seq_len_tgt, num_head, embed_dim//num_head).transpose(1, 2)\n",
    "K = linear(src_embedding).view(batch_size, seq_len_src, num_head, embed_dim//num_head).transpose(1, 2)\n",
    "V = linear(src_embedding).view(batch_size, seq_len_src, num_head, embed_dim//num_head).transpose(1, 2)\n",
    "\n",
    "print(Q.shape, V.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9dd2cf3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1.]])\n",
      "tensor([[[[False, False, False, False,  True,  True,  True]]],\n",
      "\n",
      "\n",
      "        [[[False, False, False, False, False,  True,  True]]],\n",
      "\n",
      "\n",
      "        [[[False, False,  True,  True,  True,  True,  True]]]])\n"
     ]
    }
   ],
   "source": [
    "print(generate_square_subsequent_mask(src.shape[-1]))\n",
    "print(create_causal_mask(src.shape[-1]))\n",
    "print(create_cross_attention_mask(tgt, src))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8306ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScoresShape: torch.Size([3, 8, 1, 1])\n",
      "AttentionMaskShape: torch.Size([1, 1])\n",
      "AttentionWeightShape: torch.Size([3, 8, 1, 1])\n",
      "ContextShape: torch.Size([3, 1, 128])\n",
      "tensor([[1.1111]], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Self-Attention \n",
    "\n",
    "scores = Q @ Q.transpose(-2,-1) / math.sqrt(Q.size(-1))\n",
    "print('ScoresShape:', scores.shape)\n",
    "\n",
    "attn_mask = generate_square_subsequent_mask(tgt.shape[-1])\n",
    "print('AttentionMaskShape:',attn_mask.shape)\n",
    "\n",
    "attn_weight = torch.dropout(torch.softmax(scores + attn_mask, -1), .1, train=True)\n",
    "print('AttentionWeightShape:', attn_weight.shape)\n",
    "context = (attn_weight @ Q ).transpose(1, 2).contiguous().view(batch_size, seq_len_tgt, embed_dim)\n",
    "print('ContextShape:', context.shape)\n",
    "print(attn_weight[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f67b3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = None\n",
    "\n",
    "F._canonical_mask(\n",
    "                mask=mask,\n",
    "                mask_name='attn_mask',\n",
    "                other_type=F._none_or_dtype(mask),\n",
    "                other_name=\"mask\",\n",
    "                target_type=torch.float,  \n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0164f6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScoresShape: torch.Size([3, 8, 1, 7])\n",
      "AttentionMaskShape: torch.Size([3, 1, 1, 7])\n",
      "AttentionWeightShape: torch.Size([3, 8, 1, 7])\n",
      "ContextShape: torch.Size([3, 1, 128])\n",
      "tensor([[0.7817, 0.0392, 0.1920, 0.0983, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Cross-Attention\n",
    "scores = Q @ K.transpose(-2,-1) / math.sqrt(Q.size(-1))\n",
    "print('ScoresShape:', scores.shape)\n",
    "\n",
    "attn_mask = F._canonical_mask(\n",
    "    mask=create_cross_attention_mask(tgt, src),\n",
    "    mask_name='attn_mask',\n",
    "    other_type=F._none_or_dtype(attn_mask),\n",
    "    other_name=\"mask\",\n",
    "    target_type=torch.float,\n",
    "\n",
    ")    \n",
    "print('AttentionMaskShape:',attn_mask.shape)\n",
    "\n",
    "attn_weight = clear_nan(\n",
    "    torch.dropout(torch.softmax(scores + attn_mask, -1), .1, train=True)\n",
    ")\n",
    "print('AttentionWeightShape:', attn_weight.shape)\n",
    "\n",
    "context = (\n",
    "    (attn_weight @ V ).transpose(1, 2).contiguous().view(batch_size, seq_len_tgt, embed_dim)\n",
    ")\n",
    "print('ContextShape:', context.shape)\n",
    "print(attn_weight[0][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optimus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
