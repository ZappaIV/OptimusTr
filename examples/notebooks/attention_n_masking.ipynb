{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2c6eda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from pathlib import Path\n",
    "import sys  \n",
    "\n",
    "# Get my_package directory path from Notebook\n",
    "parent_dir = str(Path().resolve().parents[1])\n",
    "\n",
    "# Add to sys.path\n",
    "sys.path.insert(0, parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de897a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import math\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c1cf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.components.functional import (generate_square_subsequent_mask, \n",
    "                               create_causal_mask,\n",
    "                               create_cross_attention_mask)\n",
    "\n",
    "def clear_nan(tensor: torch.Tensor):\n",
    "        return torch.where(torch.isnan(tensor), \n",
    "                               torch.zeros_like(tensor), \n",
    "                               tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fb89fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 128\n",
    "num_heads = 8\n",
    "hidden_dim = 200\n",
    "max_len = 5000\n",
    "batch_size = 32\n",
    "d_ff = hidden_dim\n",
    "num_head = 8\n",
    "src_seq_len = 4\n",
    "tgt_seq_len = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4dff56d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 7]) torch.Size([3, 8])\n",
      "torch.Size([3, 7, 128]) torch.Size([3, 8, 128])\n",
      "torch.Size([3, 8, 8, 16]) torch.Size([3, 8, 7, 16])\n"
     ]
    }
   ],
   "source": [
    "src = torch.tensor(\n",
    "    [[ 1,3,4,2,0,0,0],\n",
    "    [ 1,3,4,4,2,0,0],\n",
    "    [ 1,2,0,0,0,0,0]]\n",
    ")\n",
    "    \n",
    "# en_tensor = torch.randint(1, 10**4,[batch_size, seq_len], dtype=int)\n",
    "tgt = torch.tensor(\n",
    "    [[ 1,3,2,0,0,0,0,0],\n",
    "    [ 1,3,3,3,2,0,0,0],\n",
    "    [ 1,3,2,0,0,0,0,0]]\n",
    ")\n",
    "\n",
    "print(src.shape, tgt.shape)\n",
    "\n",
    "embedding = nn.Embedding(10**4, embedding_dim=embed_dim)\n",
    "src_embedding = embedding(src)\n",
    "tgt_embedding = embedding(tgt)\n",
    "\n",
    "print(src_embedding.shape, tgt_embedding.shape)\n",
    "\n",
    "batch_size, seq_len_src, embed_dim = src_embedding.shape\n",
    "_, seq_len_tgt, _ = tgt_embedding.shape\n",
    "\n",
    "linear = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "Q = linear(tgt_embedding).view(batch_size, seq_len_tgt, num_head, embed_dim//num_head).transpose(1, 2)\n",
    "K = linear(src_embedding).view(batch_size, seq_len_src, num_head, embed_dim//num_head).transpose(1, 2)\n",
    "V = linear(src_embedding).view(batch_size, seq_len_src, num_head, embed_dim//num_head).transpose(1, 2)\n",
    "\n",
    "print(Q.shape, V.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9dd2cf3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1.]])\n",
      "tensor([[[[False, False, False, False,  True,  True,  True],\n",
      "          [False, False, False, False,  True,  True,  True],\n",
      "          [False, False, False, False,  True,  True,  True],\n",
      "          [ True,  True,  True,  True,  True,  True,  True],\n",
      "          [ True,  True,  True,  True,  True,  True,  True],\n",
      "          [ True,  True,  True,  True,  True,  True,  True],\n",
      "          [ True,  True,  True,  True,  True,  True,  True],\n",
      "          [ True,  True,  True,  True,  True,  True,  True]]],\n",
      "\n",
      "\n",
      "        [[[False, False, False, False, False,  True,  True],\n",
      "          [False, False, False, False, False,  True,  True],\n",
      "          [False, False, False, False, False,  True,  True],\n",
      "          [False, False, False, False, False,  True,  True],\n",
      "          [False, False, False, False, False,  True,  True],\n",
      "          [ True,  True,  True,  True,  True,  True,  True],\n",
      "          [ True,  True,  True,  True,  True,  True,  True],\n",
      "          [ True,  True,  True,  True,  True,  True,  True]]],\n",
      "\n",
      "\n",
      "        [[[False, False,  True,  True,  True,  True,  True],\n",
      "          [False, False,  True,  True,  True,  True,  True],\n",
      "          [False, False,  True,  True,  True,  True,  True],\n",
      "          [ True,  True,  True,  True,  True,  True,  True],\n",
      "          [ True,  True,  True,  True,  True,  True,  True],\n",
      "          [ True,  True,  True,  True,  True,  True,  True],\n",
      "          [ True,  True,  True,  True,  True,  True,  True],\n",
      "          [ True,  True,  True,  True,  True,  True,  True]]]])\n"
     ]
    }
   ],
   "source": [
    "print(generate_square_subsequent_mask(src.shape[-1]))\n",
    "print(create_causal_mask(src.shape[-1]))\n",
    "print(create_cross_attention_mask(tgt, src))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8306ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScoresShape: torch.Size([3, 8, 8, 8])\n",
      "AttentionMaskShape: torch.Size([8, 8])\n",
      "AttentionWeightShape: torch.Size([3, 8, 8, 8])\n",
      "ContextShape: torch.Size([3, 8, 128])\n",
      "tensor([[1.1111, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2879, 0.8232, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2624, 0.1422, 0.7065, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2580, 0.2004, 0.2273, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1866, 0.1449, 0.1644, 0.3076, 0.3076, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1461, 0.1135, 0.1287, 0.2409, 0.2409, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1201, 0.0933, 0.1058, 0.1980, 0.1980, 0.1980, 0.1980, 0.0000],\n",
      "        [0.1019, 0.0792, 0.0898, 0.1680, 0.1680, 0.1680, 0.1680, 0.1680]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Self-Attention \n",
    "\n",
    "scores = Q @ Q.transpose(-2,-1) / math.sqrt(Q.size(-1))\n",
    "print('ScoresShape:', scores.shape)\n",
    "\n",
    "attn_mask = generate_square_subsequent_mask(tgt.shape[-1])\n",
    "print('AttentionMaskShape:',attn_mask.shape)\n",
    "\n",
    "attn_weight = torch.dropout(torch.softmax(scores + attn_mask, -1), .1, train=True)\n",
    "print('AttentionWeightShape:', attn_weight.shape)\n",
    "context = (attn_weight @ Q ).transpose(1, 2).contiguous().view(batch_size, seq_len_tgt, embed_dim)\n",
    "print('ContextShape:', context.shape)\n",
    "print(attn_weight[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f67b3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = None\n",
    "\n",
    "F._canonical_mask(\n",
    "                mask=mask,\n",
    "                mask_name='attn_mask',\n",
    "                other_type=F._none_or_dtype(mask),\n",
    "                other_name=\"mask\",\n",
    "                target_type=torch.float,  \n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0164f6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScoresShape: torch.Size([3, 8, 8, 7])\n",
      "AttentionMaskShape: torch.Size([3, 1, 8, 7])\n",
      "AttentionWeightShape: torch.Size([3, 8, 8, 7])\n",
      "ContextShape: torch.Size([3, 8, 128])\n",
      "tensor([-0.2351,  0.0437,  0.3506,  0.1932,  0.2827, -0.0188, -0.5101,  0.0163,\n",
      "         0.7627,  0.2617,  0.2117, -0.3608,  0.0725, -0.0302,  0.0799,  0.3650,\n",
      "         0.2956, -0.4211, -0.0770, -0.2030, -0.0968,  0.1160,  0.2713, -0.5688,\n",
      "         0.6390,  0.5940, -0.4506, -0.1929, -0.4062,  0.5581, -0.0551,  0.3531,\n",
      "         0.5743,  0.6006,  0.5474,  0.0894, -0.0363,  0.0387,  0.8682, -0.2338,\n",
      "         0.6029,  0.5158, -0.0798,  0.3372, -0.0107, -0.5761,  0.4835, -0.2120,\n",
      "         0.2287,  0.2663, -0.3287, -0.2782, -0.4811,  0.2382,  0.0768,  0.3470,\n",
      "        -0.3056,  0.2837, -0.0549, -0.0228, -0.0104,  0.1383, -0.3286,  0.4399,\n",
      "        -0.6087,  0.0828, -0.3126, -0.0319,  0.4788, -0.9044,  0.5025,  0.3697,\n",
      "        -0.7963, -0.4134,  0.0427, -0.4408, -0.1149,  0.3260, -0.6283, -0.4193,\n",
      "         0.0424, -0.3924,  0.0152, -0.5701, -0.1078,  0.1387, -0.2416, -0.0011,\n",
      "         0.3882, -0.3614, -0.3766, -0.2455,  0.1723,  0.1632, -0.1856,  0.5542,\n",
      "         0.2407,  0.4809, -0.2760, -0.0546,  0.0614,  0.1963, -0.2910, -0.0994,\n",
      "        -0.4712,  0.6560,  0.1417, -0.3443, -0.0357, -0.5140,  0.3895,  0.3527,\n",
      "         0.0308,  0.6375, -0.0246, -0.1476,  0.3105, -0.2973, -0.2285, -0.1672,\n",
      "        -0.2848, -0.3509,  0.0322,  0.4363, -0.0367,  0.1134,  0.0710, -0.2802],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\francesco.paletta\\AppData\\Local\\anaconda3\\envs\\optimus\\Lib\\site-packages\\torch\\nn\\functional.py:5962: UserWarning: Support for mismatched attn_mask and mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Cross-Attention\n",
    "scores = Q @ K.transpose(-2,-1) / math.sqrt(Q.size(-1))\n",
    "print('ScoresShape:', scores.shape)\n",
    "\n",
    "attn_mask = F._canonical_mask(\n",
    "    mask=create_cross_attention_mask(tgt, src),\n",
    "    mask_name='attn_mask',\n",
    "    other_type=F._none_or_dtype(attn_mask),\n",
    "    other_name=\"mask\",\n",
    "    target_type=torch.float,\n",
    "\n",
    ")    \n",
    "print('AttentionMaskShape:',attn_mask.shape)\n",
    "\n",
    "attn_weight = clear_nan(\n",
    "    torch.dropout(torch.softmax(scores + attn_mask, -1), .1, train=True)\n",
    ")\n",
    "print('AttentionWeightShape:', attn_weight.shape)\n",
    "\n",
    "context = (\n",
    "    (attn_weight @ V ).transpose(1, 2).contiguous().view(batch_size, seq_len_tgt, embed_dim)\n",
    ")\n",
    "print('ContextShape:', context.shape)\n",
    "print(attn_weight[0][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optimus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
